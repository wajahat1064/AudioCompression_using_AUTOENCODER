{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0113 00:53:51.602934 33696 deprecation.py:323] From <ipython-input-1-a9e0b4860b72>:176: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I GOT HERE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 00:54:11.267351 33696 deprecation.py:323] From c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5\n",
      "See this: F:\\tfaudio\\audiowav\\r (15).wav\n",
      "audio shape (5314197, 2)\n",
      "Returning File: F:\\tfaudio\\audiowav\\r (15).wav\n",
      "Epoch: 0\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2897.7048\n",
      "Curr Epoch: 0 Curr Batch: 0/1\n",
      "Batch Loss: 2897.7048\n",
      "Epoch Avg Loss: 2897.7048\n",
      "Epoch: 1\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2868.833\n",
      "Curr Epoch: 1 Curr Batch: 0/1\n",
      "Batch Loss: 2868.833\n",
      "Epoch Avg Loss: 2868.833\n",
      "Epoch: 2\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2834.8965\n",
      "Curr Epoch: 2 Curr Batch: 0/1\n",
      "Batch Loss: 2834.8965\n",
      "Epoch Avg Loss: 2834.8965\n",
      "Epoch: 3\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2814.4148\n",
      "Curr Epoch: 3 Curr Batch: 0/1\n",
      "Batch Loss: 2814.4148\n",
      "Epoch Avg Loss: 2814.4148\n",
      "Epoch: 4\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2808.1172\n",
      "Curr Epoch: 4 Curr Batch: 0/1\n",
      "Batch Loss: 2808.1172\n",
      "Epoch Avg Loss: 2808.1172\n",
      "Epoch: 5\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2805.7178\n",
      "Curr Epoch: 5 Curr Batch: 0/1\n",
      "Batch Loss: 2805.7178\n",
      "Epoch Avg Loss: 2805.7178\n",
      "Epoch: 6\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2800.1287\n",
      "Curr Epoch: 6 Curr Batch: 0/1\n",
      "Batch Loss: 2800.1287\n",
      "Epoch Avg Loss: 2800.1287\n",
      "Epoch: 7\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2791.9536\n",
      "Curr Epoch: 7 Curr Batch: 0/1\n",
      "Batch Loss: 2791.9536\n",
      "Epoch Avg Loss: 2791.9536\n",
      "Epoch: 8\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2785.9773\n",
      "Curr Epoch: 8 Curr Batch: 0/1\n",
      "Batch Loss: 2785.9773\n",
      "Epoch Avg Loss: 2785.9773\n",
      "Epoch: 9\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2784.0908\n",
      "Curr Epoch: 9 Curr Batch: 0/1\n",
      "Batch Loss: 2784.0908\n",
      "Epoch Avg Loss: 2784.0908\n",
      "Epoch: 10\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2783.079\n",
      "Curr Epoch: 10 Curr Batch: 0/1\n",
      "Batch Loss: 2783.079\n",
      "Epoch Avg Loss: 2783.079\n",
      "Epoch: 11\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2779.0698\n",
      "Curr Epoch: 11 Curr Batch: 0/1\n",
      "Batch Loss: 2779.0698\n",
      "Epoch Avg Loss: 2779.0698\n",
      "Epoch: 12\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2772.0159\n",
      "Curr Epoch: 12 Curr Batch: 0/1\n",
      "Batch Loss: 2772.0159\n",
      "Epoch Avg Loss: 2772.0159\n",
      "Epoch: 13\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2764.721\n",
      "Curr Epoch: 13 Curr Batch: 0/1\n",
      "Batch Loss: 2764.721\n",
      "Epoch Avg Loss: 2764.721\n",
      "Epoch: 14\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2759.0554\n",
      "Curr Epoch: 14 Curr Batch: 0/1\n",
      "Batch Loss: 2759.0554\n",
      "Epoch Avg Loss: 2759.0554\n",
      "Epoch: 15\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2754.6016\n",
      "Curr Epoch: 15 Curr Batch: 0/1\n",
      "Batch Loss: 2754.6016\n",
      "Epoch Avg Loss: 2754.6016\n",
      "Epoch: 16\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2750.2036\n",
      "Curr Epoch: 16 Curr Batch: 0/1\n",
      "Batch Loss: 2750.2036\n",
      "Epoch Avg Loss: 2750.2036\n",
      "Epoch: 17\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2745.731\n",
      "Curr Epoch: 17 Curr Batch: 0/1\n",
      "Batch Loss: 2745.731\n",
      "Epoch Avg Loss: 2745.731\n",
      "Epoch: 18\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2741.9744\n",
      "Curr Epoch: 18 Curr Batch: 0/1\n",
      "Batch Loss: 2741.9744\n",
      "Epoch Avg Loss: 2741.9744\n",
      "Epoch: 19\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2739.3975\n",
      "Curr Epoch: 19 Curr Batch: 0/1\n",
      "Batch Loss: 2739.3975\n",
      "Epoch Avg Loss: 2739.3975\n",
      "Epoch: 20\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2737.3164\n",
      "Curr Epoch: 20 Curr Batch: 0/1\n",
      "Batch Loss: 2737.3164\n",
      "Epoch Avg Loss: 2737.3164\n",
      "Epoch: 21\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2734.51\n",
      "Curr Epoch: 21 Curr Batch: 0/1\n",
      "Batch Loss: 2734.51\n",
      "Epoch Avg Loss: 2734.51\n",
      "Epoch: 22\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2730.5425\n",
      "Curr Epoch: 22 Curr Batch: 0/1\n",
      "Batch Loss: 2730.5425\n",
      "Epoch Avg Loss: 2730.5425\n",
      "Epoch: 23\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2726.2256\n",
      "Curr Epoch: 23 Curr Batch: 0/1\n",
      "Batch Loss: 2726.2256\n",
      "Epoch Avg Loss: 2726.2256\n",
      "Epoch: 24\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2722.7183\n",
      "Curr Epoch: 24 Curr Batch: 0/1\n",
      "Batch Loss: 2722.7183\n",
      "Epoch Avg Loss: 2722.7183\n",
      "Epoch: 25\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2720.3591\n",
      "Curr Epoch: 25 Curr Batch: 0/1\n",
      "Batch Loss: 2720.3591\n",
      "Epoch Avg Loss: 2720.3591\n",
      "Epoch: 26\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2718.4792\n",
      "Curr Epoch: 26 Curr Batch: 0/1\n",
      "Batch Loss: 2718.4792\n",
      "Epoch Avg Loss: 2718.4792\n",
      "Epoch: 27\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2716.2288\n",
      "Curr Epoch: 27 Curr Batch: 0/1\n",
      "Batch Loss: 2716.2288\n",
      "Epoch Avg Loss: 2716.2288\n",
      "Epoch: 28\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2713.4785\n",
      "Curr Epoch: 28 Curr Batch: 0/1\n",
      "Batch Loss: 2713.4785\n",
      "Epoch Avg Loss: 2713.4785\n",
      "Epoch: 29\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2710.7495\n",
      "Curr Epoch: 29 Curr Batch: 0/1\n",
      "Batch Loss: 2710.7495\n",
      "Epoch Avg Loss: 2710.7495\n",
      "Epoch: 30\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2708.5298\n",
      "Curr Epoch: 30 Curr Batch: 0/1\n",
      "Batch Loss: 2708.5298\n",
      "Epoch Avg Loss: 2708.5298\n",
      "Epoch: 31\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2706.7776\n",
      "Curr Epoch: 31 Curr Batch: 0/1\n",
      "Batch Loss: 2706.7776\n",
      "Epoch Avg Loss: 2706.7776\n",
      "Epoch: 32\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2705.042\n",
      "Curr Epoch: 32 Curr Batch: 0/1\n",
      "Batch Loss: 2705.042\n",
      "Epoch Avg Loss: 2705.042\n",
      "Epoch: 33\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2702.984\n",
      "Curr Epoch: 33 Curr Batch: 0/1\n",
      "Batch Loss: 2702.984\n",
      "Epoch Avg Loss: 2702.984\n",
      "Epoch: 34\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2700.694\n",
      "Curr Epoch: 34 Curr Batch: 0/1\n",
      "Batch Loss: 2700.694\n",
      "Epoch Avg Loss: 2700.694\n",
      "Epoch: 35\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2698.5034\n",
      "Curr Epoch: 35 Curr Batch: 0/1\n",
      "Batch Loss: 2698.5034\n",
      "Epoch Avg Loss: 2698.5034\n",
      "Epoch: 36\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2696.6104\n",
      "Curr Epoch: 36 Curr Batch: 0/1\n",
      "Batch Loss: 2696.6104\n",
      "Epoch Avg Loss: 2696.6104\n",
      "Epoch: 37\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2694.9312\n",
      "Curr Epoch: 37 Curr Batch: 0/1\n",
      "Batch Loss: 2694.9312\n",
      "Epoch Avg Loss: 2694.9312\n",
      "Epoch: 38\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2693.271\n",
      "Curr Epoch: 38 Curr Batch: 0/1\n",
      "Batch Loss: 2693.271\n",
      "Epoch Avg Loss: 2693.271\n",
      "Epoch: 39\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2691.544\n",
      "Curr Epoch: 39 Curr Batch: 0/1\n",
      "Batch Loss: 2691.544\n",
      "Epoch Avg Loss: 2691.544\n",
      "Epoch: 40\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2689.8262\n",
      "Curr Epoch: 40 Curr Batch: 0/1\n",
      "Batch Loss: 2689.8262\n",
      "Epoch Avg Loss: 2689.8262\n",
      "Epoch: 41\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2688.2156\n",
      "Curr Epoch: 41 Curr Batch: 0/1\n",
      "Batch Loss: 2688.2156\n",
      "Epoch Avg Loss: 2688.2156\n",
      "Epoch: 42\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2686.7153\n",
      "Curr Epoch: 42 Curr Batch: 0/1\n",
      "Batch Loss: 2686.7153\n",
      "Epoch Avg Loss: 2686.7153\n",
      "Epoch: 43\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2685.2532\n",
      "Curr Epoch: 43 Curr Batch: 0/1\n",
      "Batch Loss: 2685.2532\n",
      "Epoch Avg Loss: 2685.2532\n",
      "Epoch: 44\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2683.7788\n",
      "Curr Epoch: 44 Curr Batch: 0/1\n",
      "Batch Loss: 2683.7788\n",
      "Epoch Avg Loss: 2683.7788\n",
      "Epoch: 45\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2682.3123\n",
      "Curr Epoch: 45 Curr Batch: 0/1\n",
      "Batch Loss: 2682.3123\n",
      "Epoch Avg Loss: 2682.3123\n",
      "Epoch: 46\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2680.8975\n",
      "Curr Epoch: 46 Curr Batch: 0/1\n",
      "Batch Loss: 2680.8975\n",
      "Epoch Avg Loss: 2680.8975\n",
      "Epoch: 47\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2679.558\n",
      "Curr Epoch: 47 Curr Batch: 0/1\n",
      "Batch Loss: 2679.558\n",
      "Epoch Avg Loss: 2679.558\n",
      "Epoch: 48\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2678.2349\n",
      "Curr Epoch: 48 Curr Batch: 0/1\n",
      "Batch Loss: 2678.2349\n",
      "Epoch Avg Loss: 2678.2349\n",
      "Epoch: 49\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2676.9092\n",
      "Curr Epoch: 49 Curr Batch: 0/1\n",
      "Batch Loss: 2676.9092\n",
      "Epoch Avg Loss: 2676.9092\n",
      "Epoch: 50\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2675.6008\n",
      "Curr Epoch: 50 Curr Batch: 0/1\n",
      "Batch Loss: 2675.6008\n",
      "Epoch Avg Loss: 2675.6008\n",
      "Sample rate: 44100\n",
      "encodedshape(1062, 1800)\n",
      "Output: [[ 7.3477707e+01  6.4577866e+01  1.0814214e+01 ...  2.3098246e+04\n",
      "   1.1645315e+04 -9.3722504e+02]\n",
      " [-8.3661152e+03  2.1774602e+03  7.2459775e+03 ... -1.6125327e+02\n",
      "   1.0913843e+03  4.0661896e+02]\n",
      " [-6.2134350e+01 -1.4534984e+03  7.1217517e+02 ... -7.4994452e+02\n",
      "  -2.0445712e+02  6.6502631e+02]\n",
      " ...\n",
      " [ 1.3832479e+00  2.1197855e-02  1.4356872e-01 ...  3.0875385e-02\n",
      "   9.9974066e-02 -8.3744198e-02]\n",
      " [ 1.4182310e+00  2.9370368e-02  1.3619992e-01 ...  2.7370155e-02\n",
      "   1.0397628e-01 -9.1196567e-02]\n",
      " [ 1.2922759e+00  6.7051947e-02  1.3771275e-01 ...  1.7371118e-02\n",
      "   8.6671740e-02 -6.0075134e-02]]\n",
      "evaluation(1062, 10000)\n",
      "x_batch(1062, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aga\n",
      "Epoch: 51\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2674.338\n",
      "Curr Epoch: 51 Curr Batch: 0/1\n",
      "Batch Loss: 2674.338\n",
      "Epoch Avg Loss: 2674.338\n",
      "Epoch: 52\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2673.1316\n",
      "Curr Epoch: 52 Curr Batch: 0/1\n",
      "Batch Loss: 2673.1316\n",
      "Epoch Avg Loss: 2673.1316\n",
      "Epoch: 53\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2671.9539\n",
      "Curr Epoch: 53 Curr Batch: 0/1\n",
      "Batch Loss: 2671.9539\n",
      "Epoch Avg Loss: 2671.9539\n",
      "Epoch: 54\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2670.7708\n",
      "Curr Epoch: 54 Curr Batch: 0/1\n",
      "Batch Loss: 2670.7708\n",
      "Epoch Avg Loss: 2670.7708\n",
      "Epoch: 55\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2669.581\n",
      "Curr Epoch: 55 Curr Batch: 0/1\n",
      "Batch Loss: 2669.581\n",
      "Epoch Avg Loss: 2669.581\n",
      "Epoch: 56\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2668.4133\n",
      "Curr Epoch: 56 Curr Batch: 0/1\n",
      "Batch Loss: 2668.4133\n",
      "Epoch Avg Loss: 2668.4133\n",
      "Epoch: 57\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2667.2917\n",
      "Curr Epoch: 57 Curr Batch: 0/1\n",
      "Batch Loss: 2667.2917\n",
      "Epoch Avg Loss: 2667.2917\n",
      "Epoch: 58\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2666.2083\n",
      "Curr Epoch: 58 Curr Batch: 0/1\n",
      "Batch Loss: 2666.2083\n",
      "Epoch Avg Loss: 2666.2083\n",
      "Epoch: 59\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2665.1345\n",
      "Curr Epoch: 59 Curr Batch: 0/1\n",
      "Batch Loss: 2665.1345\n",
      "Epoch Avg Loss: 2665.1345\n",
      "Epoch: 60\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2664.053\n",
      "Curr Epoch: 60 Curr Batch: 0/1\n",
      "Batch Loss: 2664.053\n",
      "Epoch Avg Loss: 2664.053\n",
      "Epoch: 61\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2662.9692\n",
      "Curr Epoch: 61 Curr Batch: 0/1\n",
      "Batch Loss: 2662.9692\n",
      "Epoch Avg Loss: 2662.9692\n",
      "Epoch: 62\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2661.9136\n",
      "Curr Epoch: 62 Curr Batch: 0/1\n",
      "Batch Loss: 2661.9136\n",
      "Epoch Avg Loss: 2661.9136\n",
      "Epoch: 63\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2660.8782\n",
      "Curr Epoch: 63 Curr Batch: 0/1\n",
      "Batch Loss: 2660.8782\n",
      "Epoch Avg Loss: 2660.8782\n",
      "Epoch: 64\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2659.87\n",
      "Curr Epoch: 64 Curr Batch: 0/1\n",
      "Batch Loss: 2659.87\n",
      "Epoch Avg Loss: 2659.87\n",
      "Epoch: 65\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2658.871\n",
      "Curr Epoch: 65 Curr Batch: 0/1\n",
      "Batch Loss: 2658.871\n",
      "Epoch Avg Loss: 2658.871\n",
      "Epoch: 66\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2657.871\n",
      "Curr Epoch: 66 Curr Batch: 0/1\n",
      "Batch Loss: 2657.871\n",
      "Epoch Avg Loss: 2657.871\n",
      "Epoch: 67\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2656.8755\n",
      "Curr Epoch: 67 Curr Batch: 0/1\n",
      "Batch Loss: 2656.8755\n",
      "Epoch Avg Loss: 2656.8755\n",
      "Epoch: 68\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2655.8926\n",
      "Curr Epoch: 68 Curr Batch: 0/1\n",
      "Batch Loss: 2655.8926\n",
      "Epoch Avg Loss: 2655.8926\n",
      "Epoch: 69\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2654.9253\n",
      "Curr Epoch: 69 Curr Batch: 0/1\n",
      "Batch Loss: 2654.9253\n",
      "Epoch Avg Loss: 2654.9253\n",
      "Epoch: 70\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2653.9653\n",
      "Curr Epoch: 70 Curr Batch: 0/1\n",
      "Batch Loss: 2653.9653\n",
      "Epoch Avg Loss: 2653.9653\n",
      "Epoch: 71\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2653.0088\n",
      "Curr Epoch: 71 Curr Batch: 0/1\n",
      "Batch Loss: 2653.0088\n",
      "Epoch Avg Loss: 2653.0088\n",
      "Epoch: 72\n",
      "totalsongs(1, 1062, 10000)\n",
      "Song loss: 2652.0562\n",
      "Curr Epoch: 72 Curr Batch: 0/1\n",
      "Batch Loss: 2652.0562\n",
      "Epoch Avg Loss: 2652.0562\n",
      "Epoch: 73\n",
      "totalsongs(1, 1062, 10000)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#@Author: Wajahat Waheed\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "from tensorflow.contrib import ffmpeg\n",
    "from scipy.fftpack import rfft, irfft\n",
    "from glob import iglob\n",
    "from pydub import AudioSegment\n",
    "import pickle\n",
    "DATA_FILES_MP3 = 'audio'\n",
    "DATA_FILES_WAV = r'F:\\tfaudio\\audiowav' #This will be your directory of the training audio waves\n",
    "file_arr = []\n",
    "curr_batch = 0\n",
    "def convert_mp3_to_wav():\n",
    "    index = 0\n",
    "    for file in iglob(DATA_FILES_MP3 + '/*.mp3'):\n",
    "        mp3_to_wav = AudioSegment.from_mp3(file)\n",
    "        mp3_to_wav.export(DATA_FILES_WAV+'/'+str(index)+'.wav', format='wav')\n",
    "        index += 1\n",
    "def process_wav():\n",
    "    file_range = 0\n",
    "    for file in iglob(DATA_FILES_WAV +'\\*.wav'):\n",
    "        file_arr.append(file)\n",
    "def get_next_batch(curr_batch, songs_per_batch, sess):\n",
    "    wav_arr_ch1 = []\n",
    "    wav_arr_ch2 = []\n",
    "    if (curr_batch) >= (len(file_arr)):\n",
    "        curr_batch = 0\n",
    "    start_position = curr_batch * songs_per_batch\n",
    "    end_position = start_position + songs_per_batch\n",
    "    print(start_position, end_position)\n",
    "    for idx in range(start_position, end_position):\n",
    "        print(\"See this:\",file_arr[idx])\n",
    "        audio_binary = tf.read_file(file_arr[idx])\n",
    "        wav_decoder = audio_ops.decode_wav(audio_binary,desired_channels=2)\n",
    "        sample_rate, audio = sess.run([wav_decoder.sample_rate, wav_decoder.audio])\n",
    "        audio = np.array(audio)\n",
    "        print('Audio shape', str(audio.shape))\n",
    "        if len(audio[:, 0]) != 5314197: \n",
    "            print(\"yes\")\n",
    "            continue\n",
    "        wav_arr_ch1.append(rfft(audio[:,0]))\n",
    "        wav_arr_ch2.append(rfft(audio[:,1]))\n",
    "        print(\"Returning File: \" + file_arr[idx])\n",
    "    return wav_arr_ch1, wav_arr_ch2, sample_rate\n",
    "\n",
    "def save_to_wav( enc_song_ch1, enc_song_ch2, audio_arr_ch1, audio_arr_ch2, sample_rate, original_song_ch1, original_song_ch2, idty, folder, sess, dump=False):\n",
    "    #sample_rate = 44100\n",
    "    audio_arr_ch1 = irfft(np.hstack(np.hstack(audio_arr_ch1)))\n",
    "    audio_arr_ch2 = irfft(np.hstack(np.hstack(audio_arr_ch2)))\n",
    "\n",
    "    original_song_ch1 = irfft(np.hstack(np.hstack(original_song_ch1)))\n",
    "    original_song_ch2 = irfft(np.hstack(np.hstack(original_song_ch2)))\n",
    "    \n",
    "    enc_song_ch1 = irfft(np.hstack(np.hstack(enc_song_ch1)))\n",
    "    enc_song_ch2 = irfft(np.hstack(np.hstack(enc_song_ch2)))\n",
    "\n",
    "    original_song = np.hstack(np.array((original_song_ch1, original_song_ch2)).T)\n",
    "    audio_arr = np.hstack(np.array((audio_arr_ch1, audio_arr_ch2)).T)\n",
    "    enc_song = np.hstack(np.array((enc_song_ch1, enc_song_ch2)).T)\n",
    "    w = np.linspace(0, sample_rate, len(audio_arr))\n",
    "    w = w[0:len(audio_arr)]\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(w, original_song)\n",
    "    plt.savefig(str(folder) + '/original.png')\n",
    "    plt.plot(w, audio_arr)\n",
    "    plt.xlabel('sample')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.savefig(str(folder) + '/compressed' + str(idty) + '.png')\n",
    "    #plt.clf()\n",
    "    plt.show()\n",
    "    cols = 2\n",
    "    rows = math.floor(len(audio_arr)/2)\n",
    "    audio_arr = audio_arr.reshape(rows, cols)\n",
    "    original_song = original_song.reshape(rows, cols)\n",
    "    enc_song = enc_song.reshape(math.floor(len(enc_song)/2), cols) \n",
    "    if dump == True:\n",
    "        from scipy.io import wavfile\n",
    "        print('aga')\n",
    "        wavfile.write('Reconstructedsongforcontin'+str(idty)+'.wav', sample_rate, audio_arr)\n",
    "        wavfile.write('origal_songforcontin.wav', sample_rate, original_song)\n",
    "        pickle.dump(enc_song, open( \"enc_song.p\", \"wb\" ) )\n",
    "        \n",
    "def next_batch(c_batch, batch_size, sess):\n",
    "    ch1_arr = []\n",
    "    ch2_arr = []\n",
    "    wav_arr_ch1, wav_arr_ch2, sample_rate = get_next_batch(c_batch, batch_size, sess)\n",
    "\n",
    "    for sub_arr in wav_arr_ch1:\n",
    "        batch_size_ch1 = math.floor(len(sub_arr)/inputs)\n",
    "        sub_arr = sub_arr[:(batch_size_ch1*inputs)]\n",
    "        ch1_arr.append(np.array(sub_arr).reshape(batch_size_ch1, inputs))\n",
    "\n",
    "    for sub_arr in  wav_arr_ch2:\n",
    "        batch_size_ch2 = math.floor(len(sub_arr)/inputs)\n",
    "        sub_arr = sub_arr[:(batch_size_ch2*inputs)]\n",
    "        ch2_arr.append(np.array(sub_arr).reshape(batch_size_ch2, inputs))\n",
    "\n",
    "    return np.array(ch1_arr), np.array(ch2_arr), sample_rate\n",
    "\n",
    "#LOSS_OUT_FILE = 'Epoch_Loss.txt'\n",
    "process_wav()\n",
    "inputs = 10000\n",
    "#Learning rate\n",
    "lr = 0.00005\n",
    "# L2 regularization\n",
    "l2 = 0.00005\n",
    "hidden_1_size = 7000\n",
    "hidden_2_size = 4440\n",
    "hidden_3_size = 1800\n",
    "epochs = 50000\n",
    "batch_size = 1\n",
    "batches = 1\n",
    "X = tf.placeholder(tf.float32, shape=[None, inputs])\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2)\n",
    "#saver = tf.train.Saver()\n",
    "autoencoder_dnn = partial(tf.layers.dense, activation = tf.nn.elu,kernel_initializer = tf.contrib.layers.variance_scaling_initializer(),kernel_regularizer=  tf.contrib.layers.l2_regularizer(l2))\n",
    "hidden_1 = autoencoder_dnn(X, hidden_1_size)\n",
    "hidden_2 = autoencoder_dnn(hidden_1, hidden_2_size)\n",
    "hidden_4 = autoencoder_dnn(hidden_2, hidden_3_size)\n",
    "hidden_5 = autoencoder_dnn(hidden_4, hidden_2_size)\n",
    "outputs =  autoencoder_dnn(hidden_5, inputs, activation=None)\n",
    "#encoder = autoencoder_dnn(inputs, hidden_4, activation=None)\n",
    "#decoder = autoencoder_dnn(hidden_4, inputs, activation=None)\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs-X))\n",
    "reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_loss)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "##### Run training\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./model.ckpt\")\n",
    "    ch1_song, ch2_song, sample_rate = next_batch(4, batch_size, sess)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        for i in range(batches):      \n",
    "          # print(\"ch1_song\",ch1_song) \n",
    "            total_songs = np.hstack([ch1_song, ch2_song])\n",
    "            print('totalsongs' + str(total_songs.shape))\n",
    "            batch_loss = []\n",
    "            for j in range(len(total_songs)):\n",
    "                x_batch = total_songs[j]\n",
    "                _, l = sess.run([training_op, loss], feed_dict={X:x_batch})\n",
    "                batch_loss.append(l)\n",
    "                print(\"Song loss: \" + str(l))\n",
    "\n",
    "            print(\"Curr Epoch: \" + str(epoch) + \" Curr Batch: \" + str(i) + \"/\"+ str(batches))\n",
    "            print(\"Batch Loss: \" + str(np.mean(batch_loss)))\n",
    "            epoch_loss.append(np.mean(batch_loss))\n",
    "        print(\"Epoch Avg Loss: \" + str(np.mean(epoch_loss)))\n",
    "        if epoch == 50:\n",
    "            ch1_song_new, ch2_song_new, sample_rate_new = ch1_song, ch2_song, sample_rate\n",
    "            #ch1_song_new, ch2_song_new, sample_rate_new = next_batch(0, 1,sess)\n",
    "            \n",
    "          # print(\"ch1\",ch1_song_new)\n",
    "          # print(\"ch2\",ch2_song_new)\n",
    "            x_batch = np.hstack([ch1_song_new, ch2_song_new])[0]\n",
    "            print(\"Sample rate: \" + str(sample_rate_new))\n",
    "\n",
    "            orig_song = []\n",
    "            full_song = []\n",
    "            encoded_song = []\n",
    "            encoded = hidden_4.eval(feed_dict={X: x_batch})\n",
    "            print('encodedshape' + str(encoded.shape))\n",
    "            evaluation = outputs.eval(feed_dict={X: x_batch})\n",
    "            print(\"Output: \" + str(evaluation))\n",
    "            print('evaluation'+str(evaluation.shape))\n",
    "            print('x_batch'+str(x_batch.shape))\n",
    "            encoded_song.append(encoded)\n",
    "            full_song.append(evaluation)\n",
    "            orig_song.append(x_batch)\n",
    "\n",
    "            # Merge the nested arrays\n",
    "            orig_song = np.hstack(orig_song)\n",
    "            full_song = np.hstack(full_song)\n",
    "            encoded_song = np.hstack(encoded_song)\n",
    "\n",
    "            # Compute and split the channels\n",
    "            orig_song_ch1 = orig_song[:math.floor(len(orig_song)/2)]\n",
    "            orig_song_ch2 = orig_song[math.floor(len(orig_song)/2):]\n",
    "            full_song_ch1 = full_song[:math.floor(len(full_song)/2)]\n",
    "            full_song_ch2 = full_song[math.floor(len(full_song)/2):]\n",
    "            enc_song_ch1 = encoded_song[:math.floor(len(encoded_song)/2)]\n",
    "            enc_song_ch2 = encoded_song[math.floor(len(encoded_song)/2):]\n",
    "\n",
    "            # Save both the untouched song and reconstructed song to the 'output' folder\n",
    "            save_to_wav(enc_song_ch1, enc_song_ch2, full_song_ch1, full_song_ch2, sample_rate, orig_song_ch1, orig_song_ch2, epoch, 'output', sess, True)\n",
    "            #saver.save(sess, './model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
